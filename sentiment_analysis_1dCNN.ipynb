{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beLYHSUSctB2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text classification(Sentiment Analysis) 1dCNN\n",
    "    用1维CNN实现情感分类\n",
    "    运行平台 colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PRkf3HuqctB6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.11.0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# # 解压上传的review.zip文件\n",
    "# ! unzip review.zip"
   ],
   "metadata": {
    "id": "yEv2i2t7Rz_E",
    "outputId": "29acf71e-3db8-407a-c698-b87c0ef502bf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2F0ZjE2ctB9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " `./train/pos` 和 `./train/neg` 文件夹包含.txt文本文件,分别表示训练集的正类样本和负类样本"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "# 展示正类样本的数量\n",
    "len(os.listdir('./train/pos/'))"
   ],
   "metadata": {
    "id": "8XT4A4X7SEpc",
    "outputId": "d42ef52d-2e82-44da-c4ae-20fb22a44cb3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "1247"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用`tf.keras.utils.text_dataset_from_directory`从以文件夹为分类依据的文本文件，生成带标签的`tf.data.Dataset`对象。进一步生成训练集，验证集，测试集。其中测试集来自'./test/'文件夹，训练集和验证集按照80%，20%的比例从'./train/'文件夹下随机取出。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EaIr5Xp5ctB-",
    "outputId": "cb9d4fe3-d0e1-4ae6-894b-b6e0a872ad6e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1440 files belonging to 2 classes.\n",
      "Using 1152 files for training.\n",
      "Found 1440 files belonging to 2 classes.\n",
      "Using 288 files for validation.\n",
      "Found 151 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 72\n",
      "Number of batches in raw_val_ds: 18\n",
      "Number of batches in raw_test_ds: 10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"./train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"./train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"./test\", batch_size=batch_size\n",
    ")\n",
    "# 观察数据的批量数\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHTLBDKyctB_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "观察一些数据样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PqQjkR-fctB_",
    "outputId": "fdd835cc-d39b-4148-d6ca-2908a7587bd6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Author's Views on Mao are a Disgrace The author says that the reason that communist China has became skeptical and even aggressive towards American interests is because America rejected Mao, who, according to the author, wanted to reach out to America.  This is complete nonsense.  For one thing, America had reason for rejecting Mao.  Communists like Mao have harmed millions of people all over the world.  Mao literally killed millions of Chinese with his communist ideology.  Americans can have a sense of pride that we rejected Mao and wanted nothing to do with him until Richard Nixon embraced him.  Chang, the nationalist leader of China, fled to Taiwan.  The author suggests that we should never have been friends with Chang and that Mao was a far better leader.  All we have to do is look at how Taiwan turned out to see that we were right to stand with Chang, and not Mao.  Taiwan has a representative government, capitalism, a much higher standard of living than mainland China, and they produce high-quality products.\"\n",
      "0\n",
      "b'Corruption and collaboration revealed Schweizer does it again. This well documented book shocks the conscience and sends a chill up the spine of even moderately patriotic Americans. The depth and breadth of corruption and collaboration by our elites will blow your mind.'\n",
      "1\n",
      "b'\"The love of money is the route of ALL evil\" 1 Tim 6:10 - Schweizer exposes that route! Red Handed by Peter Schweizer is a meticulously documented account of the depths of human compromise when access and participation to the full potential of the Chinese economic machine is dangled in front of them. Tragically, those who have succumbed to the juicy temptations of prestige, access and obscene profits are selling out their own values and ideals in exchange for a share in the tantalizing treasure. Blinded by greed, these American patsies cannot see how much their Faustian bargains will cost their American children or their grandchildren.'\n",
      "1\n",
      "b'Awesome review of Chinese history. Very interesting reading. Excellent novel based on Chinese history. An easy way to learn their history while enjoying yourself. Great commentary and description.'\n",
      "1\n",
      "b\"Simply Awful According to James Bradley, all Americans involved with China were either drug lords or dullards. At least until 1941, because I quit reading the book at that point. His very unbalanced view adds nothing new to the study of the period, which has been done better elsewhere. What he does do is describe Asian-American relations in an often mocking and always painful-to-read style. Don't buy this book. I would let you have mine, but that would just be mean.\"\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUoGJ8nyctB_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G6158pvgctB_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\87708\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 模型超参数\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# 定义一个向量化层\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "\n",
    "# 定义一个只包含文本的数据集\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hI0GIc2FctCA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 向量化文本数据\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# 对数据进行异步预取以在 GPU 上获得最佳性能。\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmOS8JeOctCA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 建立模型\n",
    "\n",
    "建立一个简单的带有embedding层的1d-CNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tYEvAqXictCA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# 输入一个整数，表示输入词的词表索引\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# 加入词嵌入层\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "# Dropout防止过拟合\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# dense层用作分类器\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# 二分类用sigmoid将输出限制在0-1之间\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# 编译模型，选择loss函数\"binary_crossentropy\"， 优化器\"adam\"\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pezzWB78ctCB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A0iS6GA2ctCB",
    "outputId": "765732e3-4252-4e51-bb27-f4aedc488c09",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 5s 64ms/step - loss: 0.0959 - accuracy: 0.9679 - val_loss: 0.3689 - val_accuracy: 0.8993\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 5s 64ms/step - loss: 0.0205 - accuracy: 0.9913 - val_loss: 0.3603 - val_accuracy: 0.8889\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 5s 64ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.4830 - val_accuracy: 0.9097\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 5s 64ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.4796 - val_accuracy: 0.9097\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 5s 64ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.5305 - val_accuracy: 0.9132\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 5s 65ms/step - loss: 0.0016 - accuracy: 0.9991 - val_loss: 0.5360 - val_accuracy: 0.9062\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 5s 65ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5159 - val_accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 5s 65ms/step - loss: 8.6376e-04 - accuracy: 1.0000 - val_loss: 0.6358 - val_accuracy: 0.9028\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 5s 65ms/step - loss: 3.0984e-04 - accuracy: 1.0000 - val_loss: 0.6285 - val_accuracy: 0.8958\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 5s 65ms/step - loss: 1.5815e-04 - accuracy: 1.0000 - val_loss: 0.6427 - val_accuracy: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1e8a167f5b0>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 作为一个demo只训练3个轮次\n",
    "epochs = 10\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTiEOThwctCB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 在测试集上测试模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xXxv4H_9ctCB",
    "outputId": "f5ddfcac-8730-4212-bc4b-dc298c328fb8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 15ms/step - loss: 0.7841 - accuracy: 0.9073\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.7841019034385681, 0.9072847962379456]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于这只是一个demo，数据量非常小，可以发现在训练集上的误差几乎为0，而在验证集上的精确度在几轮训练后达到了瓶颈，存在过拟合现象。\n",
    "最终，模型在测试集上达到90.72%的准确度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGwk94YQctCB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 建立端到端的模型\n",
    "\n",
    "建立一个端到端的模型，使其输入为原始的文本字符串，输出为情感分析的正类概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 9ms/step - loss: 0.7841 - accuracy: 0.9073\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.7841019034385681, 0.9072847962379456]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# 加入vectorize_layer把原始字符串序列转化成单词索引\n",
    "indices = vectorize_layer(inputs)\n",
    "# indices 作为embedding+1d-CNN模型输入\n",
    "outputs = model(indices)\n",
    "\n",
    "# 端到端模型\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 在输出原始字符串的raw_test_ds数据集上做评估\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "text_classification_from_scratch",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}